\providecommand{\main}{.}
\input{\main/../../general/packages.tex}
\usepackage{booktabs}
\begin{document}

\input{\main/../../general/title.tex}

{\hypersetup{hidelinks}
	\tableofcontents  % Generates the table of contents
}
\part{Dataset}
\chapter{Data points}
First we begin looking at the dataset, the distributions of the given metrics and the statistical analysis of these data points.

\section{Retrieving the dataset}
The dataset can be downloaded from \url{https://homes.di.unimi.it/valentini/ProgettoBioinformatica1718/data/}.

\section{Composition}
\subsection{Training dataset}
In the training dataset there are 981388 data points, each one comprised of 26 metrics. The first 356 are pathogenic and all the others are negative.

\subsection{Testing dataset}
In the test dataset there are 19018 data points, still each one comprised of 26 metrics. The first 40 are pathogenic and the following are negative.

\chapter{Metrics}
\section{How the graphs are realized}
All the graphs are in triples: positives, negatives and mixed.

The normalization is done, as usual, in the following way:

\begin{figure}
	\[
		m' = \frac{\text{metric} - \mean{\text{metric values}}}{\max\crl{\text{metric values}}- \min\crl{\text{metric values}}}
	\]
	\caption{Input normalization}
\end{figure}

\subsection{Metric sample distribution}
Are realized by calculating the frequencies and estimating the density distributions parameters via MLE.

\subsection{Plot graphs}
Plot graphs are realized by sorting the values of the single metrics.

\subsection{Normalized plot graphs}
Are realized by sorting the values of the metric, with the domain and codomain normalized.

\subfile{\main/chapters/datapoints_plots}
\subfile{\main/chapters/metric_distributions}

\chapter{Data correlation}
We now proceed to try and identify eventual data correlations.

\subfile{\main/chapters/scatter_plot}
\clearpage
\subfile{\main/chapters/correlation_coefficient}
\clearpage
\subfile{\main/chapters/correlations}
\subfile{\main/chapters/data_correlations_table}
\clearpage
\subfile{\main/chapters/less_correlated_correlation_matrix}

\chapter{Dataset visualization}
After having removed the correlated metrics we can proceed to use techniques of dimensionality reduction for visualization to see if the dataset is valid for a clustering or machine learning approach:

\section{PCA}
\subsection{Training dataset visualization}
The positive data are clustered inside the negative data: a simple clustering approach would, most probably, not be enough for separating the two classes, but easily separable for any multi-parameters ML approach like networks.

The points isolated on the right are most probably errors in the realization of the dataset.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{pca/training}
	\caption{Training dataset visualization}
\end{figure}

\subsection{Testing dataset visualization}
The testing dataset is probably malformed: all the positives point are visibly clustered. A simple clustering approach such as K-Means could probably separate most of them successfully.

An ML approach with a network will probably reach sooner an high accuracy on the testing dataset than on the training dataset for this reason, being able to classify most of the positive points immediately.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{pca/testing}
	\caption{Testing dataset visualization}
\end{figure}

\subsection{Mixed dataset visualization}
The two datasets overlap correctly.
\begin{figure}
	\includegraphics[width=0.5\textwidth]{pca/mixed}
	\caption{Mixed dataset visualization}
\end{figure}

\section{TSNE}

\subsection{Training dataset visualization}
Implementation exists but it is not runnable on the current machine in a decent time.

\subsection{Testing dataset visualization}
With TSNE visualization, after 1000 iterations, the training dataset positive class appears on the border right of the cluster.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{tsne/testing}
	\caption{Testing dataset visualization}
\end{figure}

\subsection{Mixed dataset visualization}
Implementation exists but it is not runnable on the current machine in a decent time.

\chapter{Dataset issues}

\section{Possible dataset errors}
The PCA visualization shows points extremely out of the dataset cluster. It is possible that these points are errors.

\section{Biased testing dataset}
The testing dataset does not seem to reflect the training dataset distribution, but agglomerates the two classes in two separated clusters. It is therefore unhelpful when calculating the classification success in networks, as the training dataset result harder to classify that the testing one.

\subsection{Experimental proof}
To test if the testing dataset really is unhelpful in validating the network results I have tried to train an extremely small network (2 layers, 3 neurons each). The results after 300 epochs, trained on the usual training set, show a considerable success on the testing set while still low success on the training set.

\begin{figure}
	\includegraphics[width=\textwidth]{doubts.png}
	\caption{Quick ``overfitting'' on test set}
\end{figure}

\part{Network implementation}
\chapter{Model architecture}
Based on the informations extracted from the dataset structure, a small network should work best.

\begin{table}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Layer (type)}      & \textbf{Output Shape} & \textbf{Parameters number} \\
		\hline
		Input dense layer (Dense)  & (None, 22)            & 506                        \\
		\hline
		Input SeLU (Activation)    & (None, 22)            & 0                          \\
		\hline
		Hidden dense layer (Dense) & (None, 22)            & 506                        \\
		\hline
		Hidden SeLU (Activation)   & (None, 22)            & 0                          \\
		\hline
		Output dense layer (Dense) & (None, 1)             & 23                         \\
		\hline
		Sigmoid (Activation)       & (None, 1)             & 0                          \\
		\hline
	\end{tabular}
	\caption{Model summary}
\end{table}

\section{Activation function}
We'll be using the \textbf{SeLU} (\ref{selu}) activation function for most of the layers for its property of auto-normalization.

\begin{figure}
	\[
		\text{SeLU}(x) = \lambda \begin{cases}
			x                 & x > 0    \\
			\alpha e^x-\alpha & x \leq 0
		\end{cases}
	\]
	\caption{SeLU}
\end{figure}

\section{Dense layers}
Dense layers will be used through all the network:
\begin{figure}
	\includegraphics[width=0.3\textwidth]{dense_connected}
	\caption{Dense connected layer}
\end{figure}

\section{Input}
The input is a dense layer with 22 neurons, expecting as input the normalized metrics. As activation function \textbf{SeLU} will be used.

\section{Hidden layer}
A single hidden dense hidden layer with 22 neurons will be used, with \textbf{SeLU} as activation function.

\section{Drop out}
For the small dimension of the network, no drop out will probably be necessary.

\section{Output}
The output layer of the network is a dense layer with one neuron and a \textbf{sigmoid} as activation function. When \textbf{active} models the positive class, and when \textbf{inactive} models the negative class.

\begin{figure}
	\[
		\sigma(x) = \text{sigmoid}(x) = \frac{e^x}{e^x + 1}
	\]
	\caption{Sigmoid}
\end{figure}

\section{Batch Size}
Since the positive class is extremely smaller of the negative class a large batch size will be necessary: 4096 data points per batch allow for a probability that at least one of the data points is positive in the training set of the \(\approx0.77\).

\section{Loss function}
Since the task assigned to the network is a binary classification the loss function will be the \textbf{cross entropy}.

\section{Update policy}
As update policy we are going to use a form of gradient \textbf{back propagation} with \textbf{Adam} optimizer (\ref{adam}).

\section{Mathematical representation}
Since the network is extremely simple a short mathematical representation is possible:

\begin{figure}
	\[
		\text{prediction}(\bmx) = \sigma(\matr{W}_{output}\cdot\text{SeLU}(\matr{W}_{hidden}\cdot\text{SeLU}(\matr{W}_{input}\cdot\bmx)))
	\]
	\caption{Mathematical model}
\end{figure}

\section{Network model representation}
Graphical model of the network.
\begin{figure}
	\includegraphics[width=0.7\textwidth]{network}
	\caption{Model of the network}
\end{figure}

\chapter{Results}

These are the results obtained after training the network for 100 epochs.

\section{Accuracy and Loss}
\begin{table}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Metric} & \textbf{Training}     & \textbf{Testing}      \\
		\hline
		Accuracy        & 0.9998970845374103    & 0.9995267641182038    \\
		\hline
		Loss            & 0.0004865645350134752 & 0.0018388369186131115 \\
		\hline
	\end{tabular}
	\caption{Accuracy and Loss after 100 epochs}
\end{table}

\begin{figure}
	\includegraphics[width=0.7\textwidth]{results/accuracy_loss}
	\caption{Accuracy and Loss after 100 epochs}
\end{figure}

\section{ROC, AUROC, PRC and AUPRC}
\begin{table}
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Metric} & \textbf{Training}  & \textbf{Testing}   \\
		\hline
		AUROC           & 0.9982423662593878 & 0.9944672779007271 \\
		\hline
		AUPRC           & 0.8780726019818784 & 0.9337718051372246 \\
		\hline
	\end{tabular}
	\caption{Accuracy and Loss after 100 epochs}
\end{table}

\begin{figure}
	\includegraphics[width=0.7\textwidth]{results/ROC_PRC}
	\caption{ROC, AUROC, PRC and AUPRC after 100 epochs}
\end{figure}

\section{Confusion matrices}

\subsection{Training confusion matrix}
\begin{figure}
	\includegraphics[width=0.7\textwidth]{results/training_confusion}
	\caption{Training confusion matrix after 100 epochs}
\end{figure}

\subsection{Testing confusion matrix}
\begin{figure}
	\includegraphics[width=0.7\textwidth]{results/testing_confusion}
	\caption{Testing confusion matrix after 100 epochs}
\end{figure}

\section{Prediction speed}
The network classifies a data point in \(375 \mu s \pm 108 \mu s\).

This value was computer on a machine with the following carattestics:

\begin{table}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Computer Specification}       \\
		\hline
		Model Name            & MacBook Pro   \\
		\hline
		Processor Name        & Intel Core i7 \\
		\hline
		Processor Speed       & 2.3 GHz       \\
		\hline
		Number of Processors  & 1             \\
		\hline
		Total Number of Cores & 4             \\
		\hline
		L2 Cache (per Core)   & 256 KB        \\
		\hline
		L3 Cache              & 6 MB          \\
		\hline
		Memory                & 16 GB         \\
		\hline
	\end{tabular}
	\caption{Computer specifications}
\end{table}

\part{References}

\chapter{References}

\begin{description}
	\item[Adam] Adam: A Method for Stochastic Optimization: \url{https://arxiv.org/abs/1412.6980} \label{adam}
	\item[SeLU] Self-Normalizing Neural Networks: \url{https://arxiv.org/abs/1706.02515} \label{selu}
\end{description}

\input{\main/../../general/footer.tex}

\end{document}