\providecommand{\main}{.}
\input{\main/../../general/packages.tex}
\usepackage{booktabs}
\begin{document}

\input{\main/../../general/title.tex}

{\hypersetup{hidelinks}
	\tableofcontents  % Generates the table of contents
}
\part{Dataset}
\chapter{Data points}
First we begin looking at the dataset, the distributions of the given metrics and the statistical analysis of these data points.

\section{Retrieving the dataset}
The dataset can be downloaded from \url{https://homes.di.unimi.it/valentini/ProgettoBioinformatica1718/data/}.

\section{Composition}
\subsection{Training dataset}
In the training dataset there are 981388 data points, each one comprised of 26 metrics. The first 356 are pathogenic and all the others are negative.

\subsection{Testing dataset}
In the test dataset there are 19018 data points, still each one comprised of 26 metrics. The first 40 are pathogenic and the following are negative.

\chapter{Metrics}
\section{How the graphs are realized}
All the graphs are in triples: positives, negatives and mixed. All the zeros are removed as in most metrics \textit{seemed} to indicate an unknown value.

The normalization

\subsection{Metric sample distribution}
Are realized by calculating the frequencies and estimating the density distributions parameters via MLE.

\subsection{Plot graphs}
Are realized by sorting the values of the metric.

\subsection{Normalized plot graphs}
Are realized by sorting the values of the metric, with the domain and codomain normalized.

\subfile{\main/chapters/datapoints_plots}
\subfile{\main/chapters/metric_distributions}

\chapter{Data correlation}
We now proceed to try and identify eventual data correlations.

\subfile{\main/chapters/scatter_plot}
\clearpage
\subfile{\main/chapters/correlation_coefficient}

\section{Identified data correlations}
Data correlations seem to exist between:

\subsection{dbVARCount and DGVCount}
There is an extreme correlation between this two metrics:

\begin{figure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/dbVARCount_DGVCount}
		\caption{Correlation of dbVARCount and DGVCount}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/DGVCount_dbVARCount}
		\caption{Correlation of DGVCount and dbVARCount}
	\end{subfigure}
	\caption{The two metrics DGVCount and dbVARCount are strongly correlated}
\end{figure}

The two metrics seem \textbf{highly} correlated, if not the \textbf{same metric}. This means that one of the two could be removed from the dataset, as it does not add any useful information. We verify the metrics variance:

The variance of the subtraction of the two metrics is \(\Var{\text{dbVARCount} - \text{DGVCount}} = 0.0\), so the two metrics are actually the same metric.

\subsection{mamPhyloP46way and verPhyloP46way}
There is a strong correlation between this two metrics:

\begin{figure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/mamPhyloP46way_verPhyloP46way}
		\caption{Correlation of mamPhyloP46way and verPhyloP46way}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/verPhyloP46way_mamPhyloP46way}
		\caption{Correlation of verPhyloP46way and mamPhyloP46way}
	\end{subfigure}
	\caption{The two metrics verPhyloP46way and mamPhyloP46way are strongly correlated}
\end{figure}

The two metrics are \textbf{strongly} correlated, we proceed to calculate the variance of the subtraction of the two metrics: \(\Var{\text{dbVARCount} - \text{DGVCount}} = 0.02\).

It is extremely small: one of the two metrics can be removed from the database.

\subsection{mamPhastCons46way and verPhastCons46way}
There is a some correlation between this two metrics:

\begin{figure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/mamPhastCons46way_verPhastCons46way}
		\caption{Correlation of mamPhyloP46way and verPhastCons46way}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/verPhastCons46way_mamPhastCons46way}
		\caption{Correlation of mamPhastCons46way and verPhastCons46way}
	\end{subfigure}
	\caption{The two metrics mamPhastCons46way and verPhastCons46way are softly correlated}
\end{figure}

The two metrics seem \textbf{slightly} correlated, but not enough to consider removing one of the two.

We proceed to calculate the variance of the subtraction of the two metrics: \(\Var{\text{mamPhastCons46way} - \text{verPhastCons46way}} \approx 1\). The variance is too high to consider removing one of these metrics.

\subsection{verPhastCons46way and priPhastCons46way}
There is a some correlation between this two metrics:

\begin{figure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/priPhastCons46way_verPhastCons46way}
		\caption{Correlation of priPhastCons46way and verPhastCons46way}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/verPhastCons46way_priPhastCons46way}
		\caption{Correlation of verPhastCons46way and priPhastCons46way}
	\end{subfigure}
	\caption{The two metrics verPhastCons46way and priPhastCons46way are softly correlated}
\end{figure}

The two metrics seem \textbf{slightly} correlated.

We proceed to calculate the variance of the subtraction of the two metrics: \(\Var{\text{verPhastCons46way} - \text{priPhastCons46way}} \approx 1\). The variance is too high to consider removing one of these metrics.

\subsection{priPhastCons46way and mamPhastCons46way}
There is a some correlation between this two metrics:

\begin{figure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/mamPhastCons46way_priPhastCons46way}
		\caption{Correlation of mamPhastCons46way and priPhastCons46way}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\textwidth]{correlations/priPhastCons46way_mamPhastCons46way}
		\caption{Correlation of priPhastCons46way and mamPhastCons46way}
	\end{subfigure}
	\caption{The two metrics priPhastCons46way and mamPhastCons46way are softly correlated}
\end{figure}

The two metrics seem \textbf{slightly} correlated.

We proceed to calculate the variance of the subtraction of the two metrics: \(\Var{\text{priPhastCons46way} - \text{mamPhastCons46way}} \approx 1\). The variance is too high to consider removing one of these metrics.

\part{Theory}
\chapter{Input modelling}

\section{Input values}

\begin{figure}
	\[
		m' = \frac{\text{metric} - \mean{\text{metric values}}}{\max\crl{\text{metric values}}- \min\crl{\text{metric values}}}
	\]
	\caption{Input normalization}
\end{figure}

\chapter{Output modelling}
The output layer of the neural network is modelled by one neuron with a \textbf{sigmoid} as activation function. When active models the positive class, and when inactive models the negative class.

\begin{figure}
	\[
		\text{sigmoid}(x) = \frac{e^x}{e^x + 1}
	\]
	\caption{Sigmoid}
\end{figure}

\chapter{Weight initialization}
\section{Weight distribution based on input distribution}
Since input values are not from any particular distribution or hold properties such as \(\mean{X} = 0\) or \(\Var{X} = 1\) (in some metrics mean and variance are far from these values) they do not suggest to use any specific distribution.

\section{Weight distribution based on activation functions and regularization layers}
The codomain values from the activation functions, being SELU for most of the network, tend to hold the properties of \(\mean{X} = 0\) and \(\Var{X} = 1\) (\url{http://arxiv.org/abs/1706.02515}). These values are then regularized to penalize extreme weights that may appear when variance starts with a value significantly away from \(1\).

For these properties weight will be initialized by extracting them from a Gaussian with \(\mean{X} = 0\) and \(\Var{X} = 1\).

\chapter{Batch Size}

\chapter{Hidden layers}

\section{Possibility: Locally connected dense layers}
The first two layers could be locally connected dense layers, to exploit the positional information of the input values.

Other than the group of triples, input will be sorted by distribution kind so that the initial interpolations happen mostly with data from the same distribution family.

Sadly, it does not seem that with the current implementation of Keras this is possible for vector inputs, so Dense layers will be used.

\begin{figure}
	\includegraphics[width=0.3\textwidth]{locally_connected}
	\caption{Locally connected layer}
\end{figure}

For the following hidden layers we will be using dense connected layers, with a pyramidal structure (reducing the number of the neurons from 75 to 1).

\begin{figure}
	\includegraphics[width=0.3\textwidth]{dense_connected}
	\caption{Dense connected layer}
\end{figure}

\section{Activation function}
We'll be using \textbf{SELU} for the hidden layers:

\begin{figure}
	\[
		\text{selu}(x) = \lambda \begin{cases}
			x                 & x > 0    \\
			\alpha e^x-\alpha & x \leq 0
		\end{cases}
	\]
	\caption{SELU}
\end{figure}

\section{Regularization}
Regularization layers will be alternated to the dense layers to penalize weight extreme growth.

\section{Drop out}
In addition to regularization, also \textbf{drop out} of \(10\% \) of neurons per hidden layer will be applied.

\chapter{Loss function}
Since the task assigned to the network is a binary classification the loss function will be the \textbf{cross entropy}.

\chapter{Update policy}
As update policy we are going to use a form of gradient \textbf{back propagation} with \textbf{adam}.

\chapter{Network model representation}
Graphical model of the network. A version in higher resolution is available in the repository.
\begin{figure}
	\includegraphics[width=\textwidth]{network}
	\caption{Model of the network}
\end{figure}

\input{\main/../../general/footer.tex}

\chapter{References}
LatexTools does not compile references at this time.

\end{document}