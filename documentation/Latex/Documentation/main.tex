\providecommand{\main}{.}
\input{\main/../../general/packages.tex}
\usepackage{booktabs}
\begin{document}

\input{\main/../../general/title.tex}

{\hypersetup{hidelinks}
	\tableofcontents  % Generates the table of contents
}
\part{Dataset}
\chapter{Data points}
First we begin looking at the dataset, the distributions of the given metrics and the statistical analysis of these data points.

\section{Retrieving the dataset}
The dataset can be downloaded from \url{https://homes.di.unimi.it/valentini/ProgettoBioinformatica1718/data/}.

\section{Composition}
\subsection{Training dataset}
In the training dataset there are 981388 data points, each one comprised of 26 metrics. The first 356 are pathogenic and all the others are negative.

\subsection{Testing dataset}
In the test dataset there are 19018 data points, still each one comprised of 26 metrics. The first 40 are pathogenic and the following are negative.

\chapter{Metrics}
\section{How the graphs are realized}
All the graphs are in triples: positives, negatives and mixed.

The normalization is done, as usual, in the following way:

\begin{figure}
	\[
		m' = \frac{\text{metric} - \mean{\text{metric values}}}{\max\crl{\text{metric values}}- \min\crl{\text{metric values}}}
	\]
	\caption{Input normalization}
\end{figure}

\subsection{Metric sample distribution}
Are realized by calculating the frequencies and estimating the density distributions parameters via MLE.

\subsection{Plot graphs}
Plot graphs are realized by sorting the values of the single metrics.

\subsection{Normalized plot graphs}
Are realized by sorting the values of the metric, with the domain and codomain normalized.

\subfile{\main/chapters/datapoints_plots}
\subfile{\main/chapters/metric_distributions}

\chapter{Data correlation}
We now proceed to try and identify eventual data correlations.

\subfile{\main/chapters/scatter_plot}
\clearpage
\subfile{\main/chapters/correlation_coefficient}
\clearpage
\subfile{\main/chapters/correlations}
\subfile{\main/chapters/data_correlations_table}
\clearpage
\subfile{\main/chapters/less_correlated_correlation_matrix}

\chapter{Dataset visualization}
After having removed the correlated metrics we can proceed to use techniques of dimensionality reduction for visualization to see if the dataset is valid for a clustering or machine learning approach:

\section{PCA}
\subsection{Training dataset visualization}
The positive data are clustered inside the negative data: a simple clustering approach would, most probably, not be enough for separating the two classes, but easily separable for any multi-parameters ML approach like networks.

The points isolated on the right are most probably errors in the realization of the dataset.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{pca/training}
	\caption{Training dataset visualization}
\end{figure}

\subsection{Testing dataset visualization}
The testing dataset is probably malformed: all the positives point are visibly clustered. A simple clustering approach such as K-Means could probably separate most of them successfully.

An ML approach with a network will probably reach sooner an high accuracy on the testing dataset than on the training dataset for this reason, being able to classify most of the positive points immediately.

\begin{figure}
	\includegraphics[width=0.5\textwidth]{pca/testing}
	\caption{Testing dataset visualization}
\end{figure}

\subsection{Mixed dataset visualization}
The two datasets overlap correctly.
\begin{figure}
	\includegraphics[width=0.5\textwidth]{pca/mixed}
	\caption{Mixed dataset visualization}
\end{figure}

\chapter{Dataset issues}

\section{Possible dataset errors}
The PCA visualization shows points extremely out of the dataset cluster. It is possible that these points are errors.

\section{Biased testing dataset}
The testing dataset does not seem to reflect the training dataset distribution, but agglomerates the two classes in two separated clusters. It is therefore unhelpful when calculating the classification success in networks, as the training dataset result harder to classify that the testing one.

\part{Network implementation}
\chapter{Model architecture}

\section{Input}

\begin{figure}
	\[
		m' = \frac{\text{metric} - \mean{\text{metric values}}}{\max\crl{\text{metric values}}- \min\crl{\text{metric values}}}
	\]
	\caption{Input normalization}
\end{figure}

\section{Output}
The output layer of the neural network is modelled by one neuron with a \textbf{sigmoid} as activation function. When active models the positive class, and when inactive models the negative class.

\begin{figure}
	\[
		\text{sigmoid}(x) = \frac{e^x}{e^x + 1}
	\]
	\caption{Sigmoid}
\end{figure}

\section{Weight distribution based on input distribution}
Since input values are not from any particular distribution or hold properties such as \(\mean{X} = 0\) or \(\Var{X} = 1\) (in some metrics mean and variance are far from these values) they do not suggest to use any specific distribution.

\section{Weight distribution based on activation functions and regularization layers}
The codomain values from the activation functions, being SELU for most of the network, tend to hold the properties of \(\mean{X} = 0\) and \(\Var{X} = 1\) (\url{http://arxiv.org/abs/1706.02515}). These values are then regularized to penalize extreme weights that may appear when variance starts with a value significantly away from \(1\).

For these properties weight will be initialized by extracting them from a Gaussian with \(\mean{X} = 0\) and \(\Var{X} = 1\).

\section{Batch Size}

\section{Hidden layers}

\section{Possibility: Locally connected dense layers}
The first two layers could be locally connected dense layers, to exploit the positional information of the input values.

Other than the group of triples, input will be sorted by distribution kind so that the initial interpolations happen mostly with data from the same distribution family.

Sadly, it does not seem that with the current implementation of Keras this is possible for vector inputs, so Dense layers will be used.

\begin{figure}
	\includegraphics[width=0.3\textwidth]{locally_connected}
	\caption{Locally connected layer}
\end{figure}

For the following hidden layers we will be using dense connected layers, with a pyramidal structure (reducing the number of the neurons from 75 to 1).

\begin{figure}
	\includegraphics[width=0.3\textwidth]{dense_connected}
	\caption{Dense connected layer}
\end{figure}

\section{Activation function}
We'll be using \textbf{SELU} for the hidden layers:

\begin{figure}
	\[
		\text{selu}(x) = \lambda \begin{cases}
			x                 & x > 0    \\
			\alpha e^x-\alpha & x \leq 0
		\end{cases}
	\]
	\caption{SELU}
\end{figure}

\section{Regularization}
Regularization layers will be alternated to the dense layers to penalize weight extreme growth.

\section{Drop out}
In addition to regularization, also \textbf{drop out} of \(10\% \) of neurons per hidden layer will be applied.

\section{Loss function}
Since the task assigned to the network is a binary classification the loss function will be the \textbf{cross entropy}.

\section{Update policy}
As update policy we are going to use a form of gradient \textbf{back propagation} with \textbf{adam}.

\section{Network model representation}
Graphical model of the network. A version in higher resolution is available in the repository.
\begin{figure}
	\includegraphics[width=\textwidth]{network}
	\caption{Model of the network}
\end{figure}

\input{\main/../../general/footer.tex}

\chapter{References}
LatexTools does not compile references at this time.

\end{document}